"""
LLM Factory - Crea instancias de LLM segÃºn configuraciÃ³n.
Soporta OpenAI (GPT-4, GPT-4o) y Google (Gemini Pro, Gemini Flash).
"""
import logging
from typing import Type

from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.language_models import BaseChatModel

from app.config import settings
from app.llm.base import LLMInterface
from app.llm.tracked_llm import TrackedLLM


logger = logging.getLogger(__name__)


class OpenAIProvider(LLMInterface):
    """
    Proveedor OpenAI (GPT-4, GPT-4o, GPT-4-turbo, etc.)
    
    Modelos soportados:
    - gpt-4o (recomendado)
    - gpt-4-turbo
    - gpt-4
    - gpt-3.5-turbo
    """
    
    @property
    def provider_name(self) -> str:
        return "OpenAI"
    
    def validate_config(self) -> bool:
        """Valida que OPENAI_API_KEY estÃ© configurada."""
        if not settings.OPENAI_API_KEY:
            raise ValueError(
                "OPENAI_API_KEY no estÃ¡ configurada en .env. "
                "ObtÃ©n tu API key en https://platform.openai.com/api-keys"
            )
        return True
    
    def get_llm(self) -> ChatOpenAI:
        """Retorna instancia de ChatOpenAI configurada."""
        self.validate_config()
        
        return ChatOpenAI(
            model=settings.LLM_MODEL,
            temperature=settings.LLM_TEMPERATURE,
            max_tokens=settings.LLM_MAX_TOKENS,
            api_key=settings.OPENAI_API_KEY
        )


class GoogleProvider(LLMInterface):
    """
    Proveedor Google (Gemini Pro, Gemini Flash, etc.)
    
    Modelos soportados:
    - gemini-2.0-flash-exp (recomendado)
    - gemini-1.5-pro
    - gemini-1.5-flash
    - gemini-pro
    """
    
    @property
    def provider_name(self) -> str:
        return "Google"
    
    def validate_config(self) -> bool:
        """Valida que GOOGLE_API_KEY estÃ© configurada."""
        if not settings.GOOGLE_API_KEY:
            raise ValueError(
                "GOOGLE_API_KEY no estÃ¡ configurada en .env. "
                "ObtÃ©n tu API key en https://aistudio.google.com/app/apikey"
            )
        return True
    
    def get_llm(self) -> ChatGoogleGenerativeAI:
        """Retorna instancia de ChatGoogleGenerativeAI configurada."""
        self.validate_config()
        
        return ChatGoogleGenerativeAI(
            model=settings.LLM_MODEL,
            temperature=settings.LLM_TEMPERATURE,
            max_output_tokens=settings.LLM_MAX_TOKENS,
            google_api_key=settings.GOOGLE_API_KEY
        )


# Registro de providers disponibles
PROVIDERS: dict[str, Type[LLMInterface]] = {
    "openai": OpenAIProvider,
    "google": GoogleProvider,
}


def get_llm() -> BaseChatModel:
    """
    Factory que retorna el LLM configurado segÃºn LLM_PROVIDER.
    
    Lee la configuraciÃ³n de settings y retorna la instancia
    apropiada (OpenAI o Google Gemini).
    
    Returns:
        BaseChatModel: Instancia del LLM configurado
        
    Raises:
        ValueError: Si LLM_PROVIDER no es vÃ¡lido
        
    Uso:
        llm = get_llm()  # Retorna OpenAI o Google segÃºn .env
        response = await llm.ainvoke("Hola!")
    """
    provider_class = PROVIDERS.get(settings.LLM_PROVIDER)
    
    if not provider_class:
        available = list(PROVIDERS.keys())
        raise ValueError(
            f"LLM_PROVIDER '{settings.LLM_PROVIDER}' no vÃ¡lido. "
            f"Opciones disponibles: {available}"
        )
    
    provider = provider_class()
    return provider.get_llm()


def validate_llm_config() -> BaseChatModel:
    """
    Valida configuraciÃ³n LLM al iniciar la aplicaciÃ³n.
    Imprime informaciÃ³n de diagnÃ³stico y retorna el LLM si es vÃ¡lido.
    
    Returns:
        BaseChatModel: Instancia del LLM si la configuraciÃ³n es vÃ¡lida
        
    Raises:
        ValueError: Si hay errores de configuraciÃ³n
    """
    print("ðŸ¤– Configurando LLM...")
    print(f"   Provider: {settings.LLM_PROVIDER}")
    print(f"   Model: {settings.LLM_MODEL}")
    print(f"   Temperature: {settings.LLM_TEMPERATURE}")
    print(f"   Max Tokens: {settings.LLM_MAX_TOKENS}")
    
    try:
        llm = get_llm()
        print("   âœ… LLM configurado correctamente")
        logger.info(
            f"LLM configurado: provider={settings.LLM_PROVIDER}, "
            f"model={settings.LLM_MODEL}"
        )
        return llm
    except ValueError as e:
        print(f"   âŒ Error en configuraciÃ³n LLM: {e}")
        logger.error(f"Error configurando LLM: {e}")
        raise
    except Exception as e:
        print(f"   âŒ Error inesperado: {e}")
        logger.error(f"Error inesperado configurando LLM: {e}")
        raise


def get_provider_info() -> dict:
    """
    Retorna informaciÃ³n sobre el provider LLM actual.
    Ãštil para endpoints de health/status.
    
    Returns:
        dict: InformaciÃ³n del provider configurado
    """
    return {
        "provider": settings.LLM_PROVIDER,
        "model": settings.LLM_MODEL,
        "temperature": settings.LLM_TEMPERATURE,
        "max_tokens": settings.LLM_MAX_TOKENS,
        "available_providers": list(PROVIDERS.keys())
    }


def get_tracked_llm(
    node_name: str,
    inference_type: str = "general"
) -> TrackedLLM:
    """
    Factory que retorna un TrackedLLM wrapper para tracking de tokens.
    
    Args:
        node_name: Nombre del nodo (ej: "manager", "financiero_planificar")
        inference_type: Tipo de inferencia (ej: "planning", "synthesis", "specialist")
    
    Returns:
        TrackedLLM: LLM envuelto con tracking de tokens
    
    Uso:
        llm = get_tracked_llm("manager", "planning")
        response = await llm.ainvoke("...")
    """
    base_llm = get_llm()
    return TrackedLLM(
        llm=base_llm,
        node_name=node_name,
        inference_type=inference_type
    )



